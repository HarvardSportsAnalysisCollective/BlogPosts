{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Test Decay-K ELO Model Against 538's ELO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "data538 = pd.read_csv('nba_elo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Fit ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_post_elo(home_team, home_team_elo, away_team, away_team_elo, mov, decay):\n",
    "    \n",
    "    home_post_elo = get_k(mov, home_team_elo, away_team_elo, decay)*(int(mov>0) - get_win_prob(home_team_elo, away_team_elo)) + home_team_elo\n",
    "    away_post_elo = away_team_elo + (home_team_elo - home_post_elo)\n",
    "    \n",
    "    return home_post_elo, away_post_elo\n",
    "    \n",
    "def get_win_prob(home_team_elo, away_team_elo):\n",
    "    home_team_elo += 100\n",
    "    return 1.0/(1.0+(10.0**((away_team_elo-home_team_elo)/400)))\n",
    "\n",
    "def get_k(mov, team_elo, opp_elo, decay):\n",
    "    \n",
    "    if mov > 0: elo_dif = team_elo-opp_elo\n",
    "    else: elo_dif = opp_elo-team_elo\n",
    "    \n",
    "    K = 20.0*((np.abs(mov)+3.0)**.8)\n",
    "    K = K/(7.5 + .006*elo_dif)\n",
    "    return K*decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details:\n",
    " - Trained on 2010 - 2014 seasons\n",
    " - Validated on 2015 - 2016 seasons\n",
    " - Tested/Comppared to 538's Model on 2017 - 2018 seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Just Seasons 2010 and on\n",
    "games = data538[data538['season'] >= 2010]\n",
    "games = games[games['season'] < 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters to Tune\n",
    "season_start_epsilons = [.8, .9, .95, .98, 1.0, 1.02, 1.05, 1.1, 1.2]\n",
    "epsilon_decays = [.6, .7, .8, .9, .95, .98, 1.0, 1.02, 1.05, 1.1, 1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Decay ELOs\n",
    "\n",
    "VALIDATE_YEAR = 2015\n",
    "TEST_YEAR = 2017\n",
    "\n",
    "val_results_dict_list = []\n",
    "test_results_dict_list = []\n",
    "\n",
    "# For Each Combination of start_epsilon and epsilon_decay\n",
    "for season_start_epsilon in season_start_epsilons:\n",
    "    for epsilon_decay in epsilon_decays:\n",
    "    \n",
    "        epsilon = season_start_epsilon\n",
    "\n",
    "        elo_dict = {}\n",
    "\n",
    "        val_dict_list = []\n",
    "        test_dict_list = []\n",
    "\n",
    "        # For Each Game\n",
    "        for i in range(games.shape[0]):\n",
    "\n",
    "            # Get Game Info\n",
    "            row = games.iloc[i]\n",
    "            team = row.team1\n",
    "            opp = row.team2\n",
    "            mov = row.score1 - row.score2\n",
    "\n",
    "            # Seasonal ELO Adjustment for Every Team\n",
    "            if i > 0 and row.season != games.iloc[i-1].season:\n",
    "                for k in elo_dict.keys():\n",
    "                    elo_dict[k] = .75*elo_dict[k] + .25*1505\n",
    "                    \n",
    "                # Reset Epsilon\n",
    "                epsilon = season_start_epsilon\n",
    "\n",
    "            # If team's first game, use 538's elo to start\n",
    "            if team not in elo_dict.keys():\n",
    "                elo_dict[team] = row['elo1_pre']\n",
    "            if opp not in elo_dict.keys():\n",
    "                elo_dict[opp] = row['elo2_pre']\n",
    "\n",
    "            # Get Pre-Game ELO Estimates\n",
    "            team_pre = elo_dict[team]\n",
    "            opp_pre = elo_dict[opp]\n",
    "\n",
    "            # Adjust Epsilon Every 100 Games\n",
    "            if i % 100 == 0:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "            # Update ELOs based on game results    \n",
    "            elo_dict[team], elo_dict[opp] = get_post_elo(team, team_pre, opp, opp_pre, mov, epsilon)\n",
    "\n",
    "            # Model Validation\n",
    "            if row.season >= VALIDATE_YEAR and row.season < TEST_YEAR:\n",
    "                \n",
    "                # Build DataFrame with Post-Game ELOs\n",
    "                team_post = elo_dict[team]\n",
    "                opp_post = elo_dict[opp]\n",
    "                val_dict_list.append({'team1': team, 'team2': opp, 'team1_pre': team_pre, 'team2_pre': opp_pre, 'team1_post': team_post, 'team2_post': opp_post, 'MOV': mov})\n",
    "            \n",
    "        ########################\n",
    "        # Validation DataFrame\n",
    "        ########################\n",
    "        '''Choose the best model based on the validation set'''\n",
    "        \n",
    "        val_df = pd.DataFrame(val_dict_list)\n",
    "        \n",
    "        preds = []\n",
    "        outcomes = []\n",
    "        \n",
    "        # For Each Game in Validation Set\n",
    "        for i in range(val_df.shape[0]):\n",
    "\n",
    "            # Get Game Info\n",
    "            row = val_df.iloc[i]\n",
    "            mov = row.MOV \n",
    "            team_elo = row['team1_pre']\n",
    "            opp_elo = row['team2_pre']\n",
    "\n",
    "            # Get Predictions and Results\n",
    "            preds.append(get_win_prob(team_elo, opp_elo) > .5)\n",
    "            outcomes.append(mov > 0)\n",
    "\n",
    "        # True if Success, Otherwise False\n",
    "        success_list = [preds[j] == outcomes[j] for j in range(len(preds))]\n",
    "        \n",
    "        # Make DataFrame of Validation Results\n",
    "        val_results_dict_list.append({'accuracy': np.mean(success_list), 'season_start_ep': season_start_epsilon, 'epsilon_decay': epsilon_decay})\n",
    "        \n",
    "val_results_df = pd.DataFrame(val_results_dict_list)\n",
    "\n",
    "print('Best Validation Accuracy: ', np.max(val_results_df['accuracy']))\n",
    "print('Best start_epsilon: ', val_results_df.loc[np.argmax(val_results_df['accuracy'])].season_start_ep)\n",
    "print('Best epsilon_decay: ', val_results_df.loc[np.argmax(val_results_df['accuracy'])].epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Fit Model w/ Best Validation Results\n",
    "VALIDATE_YEAR = 2015\n",
    "TEST_YEAR = 2017\n",
    "\n",
    "test_results_dict_list = []\n",
    "\n",
    "# Get Hyperparameters from Validated Model\n",
    "season_start_epsilon = val_results_df.loc[np.argmax(val_results_df['accuracy'])].season_start_ep\n",
    "epsilon_decay = val_results_df.loc[np.argmax(val_results_df['accuracy'])].epsilon_decay\n",
    "\n",
    "######################\n",
    "# Re-Fit Model\n",
    "######################\n",
    "\n",
    "epsilon = season_start_epsilon\n",
    "\n",
    "elo_dict = {}\n",
    "\n",
    "test_dict_list = []\n",
    "all_dict_list = []\n",
    "\n",
    "# For Each Game\n",
    "for i in range(games.shape[0]):\n",
    "\n",
    "    # Get Game Info\n",
    "    row = games.iloc[i]\n",
    "    team = row.team1\n",
    "    opp = row.team2\n",
    "    mov = row.score1 - row.score2\n",
    "\n",
    "    # Seasonal ELO Adjustment for Every Team\n",
    "    if i > 0 and row.season != games.iloc[i-1].season:\n",
    "        for k in elo_dict.keys():\n",
    "            elo_dict[k] = .75*elo_dict[k] + .25*1505\n",
    "\n",
    "        # Reset Epsilon\n",
    "        epsilon = season_start_epsilon\n",
    "\n",
    "    # If team's first game, use 538's elo to start\n",
    "    if team not in elo_dict.keys():\n",
    "        elo_dict[team] = row['elo1_pre']\n",
    "    if opp not in elo_dict.keys():\n",
    "        elo_dict[opp] = row['elo2_pre']\n",
    "\n",
    "    # Get Pre-Game ELO Estimates\n",
    "    team_pre = elo_dict[team]\n",
    "    opp_pre = elo_dict[opp]\n",
    "\n",
    "    # Adjust Epsilon Every 100 Games\n",
    "    if i % 100 == 0:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Update ELOs based on game results    \n",
    "    elo_dict[team], elo_dict[opp] = get_post_elo(team, team_pre, opp, opp_pre, mov, epsilon)\n",
    "    \n",
    "    # Build DataFrame with new post-predictions\n",
    "    team_post = elo_dict[team]\n",
    "    opp_post = elo_dict[opp]\n",
    "\n",
    "    all_dict_list.append({'date': row.date, 'season': row.season,\n",
    "                          'team1': team, 'team2': opp, 'elo1_pre': team_pre, 'elo2_pre': opp_pre, 'elo1_post': team_post, 'elo2_post': opp_post,\n",
    "                          'elo1_pre_538': row['elo1_pre'], 'elo2_pre_538': row['elo2_pre'],\n",
    "                          'carm_elo1_pre_538': row['carm-elo1_pre'], 'carm_elo2_pre_538': row['carm-elo2_pre'],\n",
    "                          'carmelo1_pre_538': row['carmelo1_pre'], 'carmelo2_pre_538': row['carmelo2_pre'],\n",
    "                          'score1': row.score1, 'score2': row.score2,\n",
    "                          'MOV': mov})\n",
    "\n",
    "    # Final Model Testing\n",
    "    if row.season >= TEST_YEAR:\n",
    "\n",
    "        # Build DataFrame with new post-predictions\n",
    "        team_post = elo_dict[team]\n",
    "        opp_post = elo_dict[opp]\n",
    "\n",
    "        test_dict_list.append({'team1': team, 'team2': opp, 'team1_pre': team_pre, 'team2_pre': opp_pre, 'team1_post': team_post, 'team2_post': opp_post, 'MOV': mov})\n",
    "\n",
    "########################\n",
    "# Test DataFrame\n",
    "########################\n",
    "'''Get Results on Test Set for Final Comparison'''\n",
    "\n",
    "test_df= pd.DataFrame(test_dict_list)\n",
    "\n",
    "preds = []\n",
    "outcomes = []\n",
    "\n",
    "# For Each Game in Test Set\n",
    "for i in range(test_df.shape[0]):\n",
    "\n",
    "    # Get Game Info\n",
    "    row = test_df.iloc[i]\n",
    "    mov = row.MOV \n",
    "    team_elo = row['team1_pre']\n",
    "    opp_elo = row['team2_pre']\n",
    "\n",
    "    preds.append(get_win_prob(team_elo, opp_elo) > .5)\n",
    "    outcomes.append(mov > 0)\n",
    "\n",
    "# True if Success, Otherwise False\n",
    "success_list = [preds[j] == outcomes[j] for j in range(len(preds))]\n",
    "\n",
    "# Print Classification Accuracy\n",
    "print('Accuracy of Chosen Model (on 2017 and 2018): ', np.mean(success_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write New .csv w/ new ELOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(all_dict_list).to_csv('all_elos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
